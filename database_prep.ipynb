{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before Running the Program - Preparation Part 1\n",
    "#### AVA Dataset Preparation\n",
    "#### Before run the .ipynb file, please follow the link and download AVA image database first.\n",
    "#### Link to download AVA: http://academictorrents.com/details/71631f83b11d3d79d8f84efe0a7e12f0ac001460\n",
    "#### Note: Need a Bittorrent client to download all the files\n",
    "#### After finishing downloading, we have an folder called \"AVA_dataset\" which contains all the 7z files of images\n",
    "#### Unzip all the files to the folder \"AVA_dataset\"\n",
    "#### Move the AVA_dataset which contains all AVA images to the folder \"database\" contained in our project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before Running the Program - Preparation Part 2\n",
    "#### Download the VOC test set from PASCAL VOC Challenge performance evaluation and download server\n",
    "#### Link to download: http://host.robots.ox.ac.uk:8080/\n",
    "#### Note: You will need to register if it is your first time to download the test dataset\n",
    "#### After login with your username and password, click the link of \"PASCAL Visual Object Classes Challenge 2012 (VOC2012)\" to download the database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program Start ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import shutil\n",
    "import glob\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import urllib\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from clint.textui import progress\n",
    "from sys import stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sys.version_info[0] < 3:\n",
    "    raise Exception(\"Must be using Python 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloader(url,parameters, file_name):\n",
    "    session = requests.Session()\n",
    "    response = session.get(url,params=parameters, stream=True)\n",
    "    if parameters is not None:\n",
    "        token = None\n",
    "        for key, value in response.cookies.items():\n",
    "            if key.startswith('download_warning'):\n",
    "                token = value\n",
    "        if token:\n",
    "            params = {'id': '0B7sjGeF4f3FYQUVlZ3ZOai1ieEU', 'confirm': token}\n",
    "            response = session.get('https://docs.google.com/uc?export=download', params=params, stream=True)\n",
    "    with open(file_name, 'wb') as f:\n",
    "        total_size = 0\n",
    "        if file_name != 'emotic.zip':\n",
    "            total_size = int(response.headers.get('content-length'))\n",
    "        else:\n",
    "            total_size = 3602935747\n",
    "        with tqdm(total = int(total_size/32768), position=0, leave=True) as pbar:\n",
    "            for chunk in response.iter_content(32768):\n",
    "                f.write(chunk)\n",
    "                pbar.update()\n",
    "            pbar.close()\n",
    "    print(\"%s downloading - Done!\" %file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download VOC Train Validation Set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61025it [02:57, 344.43it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voc.tar downloading - Done!\n"
     ]
    }
   ],
   "source": [
    "url = 'http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar'\n",
    "parameters = None\n",
    "file_name = 'voc.tar'\n",
    "downloader(url, parameters, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract VOC Train Validation tar File, Rename Images, and Move Image Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17126it [00:31, 538.74it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOC - Done!\n"
     ]
    }
   ],
   "source": [
    "tar = tarfile.open('voc.tar')\n",
    "tar.extractall('database/voc_emotic_ava')\n",
    "files = \"database/voc_emotic_ava/VOCdevkit/VOC2012/JPEGImages/*\"\n",
    "with tqdm(total = int(len(glob.glob(files))), position=0, leave=True) as pbar:\n",
    "    for path in glob.glob(files):\n",
    "        name = path.split(\"/\")[-1]\n",
    "        newp1 = path.replace(name,\"\")+\"JPEGImages__\"+name\n",
    "        newp2 = path.replace(name, \"\")+\"VOC2012__\"+name\n",
    "        os.rename(path, newp1)\n",
    "        shutil.copy(newp1, 'database/voc_emotic_ava/')\n",
    "        os.rename(newp1, newp2)\n",
    "        shutil.copy(newp2, 'database/voc_emotic_ava/')\n",
    "        os.remove(newp2)\n",
    "        pbar.update()\n",
    "    pbar.update()\n",
    "    pbar.close()\n",
    "shutil.rmtree('database/voc_emotic_ava/VOCdevkit')\n",
    "print(\"VOC - Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract VOC Test tar File, Rename Images, and Move Image Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16136it [00:23, 686.37it/s]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOC_test_set_2012 - Done!\n"
     ]
    }
   ],
   "source": [
    "tar = tarfile.open('voc_test.tar')\n",
    "tar.extractall('database/voc_emotic_ava')\n",
    "files = \"database/voc_emotic_ava/VOCdevkit/VOC2012/JPEGImages/*\"\n",
    "with tqdm(total = int(len(glob.glob(files))), position=0, leave=True) as pbar:\n",
    "    for path in glob.glob(files):\n",
    "        name = path.split(\"/\")[-1]\n",
    "        newp1 = path.replace(name,\"\")+\"JPEGImages__\"+name\n",
    "        newp2 = path.replace(name, \"\")+\"VOC2012__\"+name\n",
    "        os.rename(path, newp1)\n",
    "        shutil.copy(newp1, 'database/voc_emotic_ava/')\n",
    "        os.rename(newp1, newp2)\n",
    "        shutil.copy(newp2, 'database/voc_emotic_ava/')\n",
    "        os.remove(newp2)\n",
    "        pbar.update()\n",
    "    pbar.update()\n",
    "    pbar.close()\n",
    "shutil.rmtree('database/voc_emotic_ava/VOCdevkit')\n",
    "print(\"VOC_test_set_2012 - Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Emotic..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "109956it [01:12, 1518.22it/s]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotic.zip downloading - Done!\n"
     ]
    }
   ],
   "source": [
    "url = 'https://docs.google.com/uc?export=download'\n",
    "parameters = {'id': '0B7sjGeF4f3FYQUVlZ3ZOai1ieEU'}\n",
    "file_name = 'emotic.zip'\n",
    "downloader(url, parameters, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Emotic Zip File, Rename Images, and Move Image Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:03<00:13,  3.36s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:03<00:07,  2.42s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [00:03<00:01,  1.74s/it]\u001b[A\n",
      "100%|██████████| 5/5 [00:08<00:00,  1.65s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotic - Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "zipfile.ZipFile('emotic.zip', 'r').extractall('database/voc_emotic_ava')\n",
    "folders = os.listdir(\"database/voc_emotic_ava/emotic/\")\n",
    "with tqdm(total = len(folders), position=1, leave=True) as pbar:\n",
    "    for folder in folders: \n",
    "        files = \"database/voc_emotic_ava/emotic/\"+folder + \"/images/*\"\n",
    "        for path in glob.glob(files):\n",
    "            name = path.split(\"/\")[-1]\n",
    "            newp = path.replace(name,\"\")+\"EMOTIC__\"+name\n",
    "            os.rename(path, newp)\n",
    "            shutil.copy(newp, 'database/voc_emotic_ava/')\n",
    "            os.remove(newp)\n",
    "        pbar.update()\n",
    "    pbar.close()\n",
    "shutil.rmtree('database/voc_emotic_ava/emotic')\n",
    "shutil.rmtree('database/voc_emotic_ava/__MACOSX')\n",
    "print(\"Emotic - Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Blur Detection Dataset ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3731it [00:17, 208.82it/s]                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blur.zip downloading - Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "url = 'http://www.cse.cuhk.edu.hk/~leojia/projects/dblurdetect/data/BlurDatasetImage.zip'\n",
    "parameters = None\n",
    "file_name = 'blur.zip'\n",
    "downloader(url, parameters, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Emotic Zip File, Rename Images, and Move Image Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blur dataset - Done!\n"
     ]
    }
   ],
   "source": [
    "zipfile.ZipFile('blur.zip','r').extractall('database/blur_dataset')\n",
    "for path in glob.glob('database/blur_dataset/image/*'):\n",
    "    shutil.copy(path, 'database/blur_dataset/')\n",
    "    os.remove(path)\n",
    "shutil.rmtree('database/blur_dataset/image')\n",
    "print(\"Blur dataset - Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename and Move AVA Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir('database/AVA_dataset') == True:\n",
    "    files = 'database/AVA_dataset/images/*'\n",
    "    for path in tqdm(glob.glob(files)):\n",
    "        name = path.split(\"/\")[-1]\n",
    "        newp = path.replace(name, \"\")+\"AVA__\"+name\n",
    "        os.rename(path, newp)\n",
    "        shutil.move(newp, 'database/voc_emotic_ava/')\n",
    "    shutil.rmtree('database/voc_emotic_ava/AVA_dataset')\n",
    "    print(\"AVA - Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Image Not Been Used ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ims = pd.read_csv('labels_image.csv')\n",
    "ims = df_ims['name']\n",
    "ims = ims.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = os.listdir('database')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning - Done!\n"
     ]
    }
   ],
   "source": [
    "for folder in folders:\n",
    "    files = 'database/'+folder+\"/*\"\n",
    "    for path in glob.glob(files):\n",
    "        name = path.replace(\"database/\",\"\")\n",
    "        if name not in ims:\n",
    "            os.remove(path)\n",
    "print(\"Cleaning - Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling Patches from Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if os.path.isdir('database/patches') == False:\n",
    "        os.mkdir('database/patches')\n",
    "except OSError:\n",
    "    print (\"Creation of the directory %s failed\" % 'patch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_sampling(im_path, patch_index, x, y, width, height):\n",
    "    img=cv2.imread(im_path)\n",
    "    patch = img[y:y+height, x:x+width]\n",
    "    name = im_path.split('/')[-1]\n",
    "    p_path = 'database/patches/'+name+\"_\"+\"patch_\"+str(patch_index)+\".jpg\"\n",
    "    cv2.imwrite(p_path,patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coor = pd.read_csv('all_patches.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch sampling - Done!\n"
     ]
    }
   ],
   "source": [
    "for folder in folders:\n",
    "    if folder != 'patches':\n",
    "        im_paths = 'database/'+folder+\"/*\"\n",
    "        im_name = im_path.split('/')[-1]\n",
    "        patch_name = \"patches/\"+im_name\n",
    "        row = df_coor.loc[df_coor['name_patch']==patch_name]\n",
    "        if len(row) == 1:\n",
    "            x1 = int(row['top_patch_1'].values.tolist()[0])\n",
    "            y1 = int(row['left_patch_1'].values.tolist()[0])\n",
    "            x2 = int(row['top_patch_2'].values.tolist()[0])\n",
    "            y2 = int(row['left_patch_2'].values.tolist()[0])\n",
    "            x3 = int(row['top_patch_3'].values.tolist()[0])\n",
    "            y3 = int(row['left_patch_3'].values.tolist()[0])\n",
    "            h1 = int(row['height_patch_1'].values.tolist()[0])\n",
    "            w1 = int(row['width_patch_1'].values.tolist()[0])\n",
    "            h2 = int(row['height_patch_2'].values.tolist()[0])\n",
    "            w2 = int(row['width_patch_2'].values.tolist()[0])\n",
    "            h3 = int(row['height_patch_3'].values.tolist()[0])\n",
    "            w3 = int(row['width_patch_3'].values.tolist()[0])\n",
    "            patch_sampling(im_path, 1, x1, y1, w1, h1)\n",
    "            patch_sampling(im_path, 2, x2, y2, w2, h2)\n",
    "            patch_sampling(im_path, 3, x3, y3, w3, h3)\n",
    "print(\"Patch sampling - Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations! It's All Set! Now all you need is in the database folder. :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
